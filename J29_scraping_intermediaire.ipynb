{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping - Exercices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HfzVA9AB3iN6"
   },
   "source": [
    "## Obtenir les derniers articles du Monde (**)\n",
    "\n",
    "* Obtenir le titre et le chapeau (description) des 10 derniers articles sur https://www.lemonde.fr/actualite-en-continu/ (*)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.lemonde.fr/actualite-en-continu/\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "content = response.content\n",
    "\n",
    "soup = BeautifulSoup(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lister toutes les balises h3 avec la classe teaser__title\n",
    "soup.find_all(\"h3\", \"teaser__title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#récupérer les 10 premières balises h3 teaser__title\n",
    "soup.find_all(\"h3\", \"teaser__title\")[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#récupérer le titre contenu dans la première balise\n",
    "soup.find_all(\"h3\", \"teaser__title\")[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#récupérer les 10 premiers titres\n",
    "#méthode 1: avec une boucle for\n",
    "\n",
    "top_titles = []\n",
    "\n",
    "for balise in soup.find_all(\"h3\", \"teaser__title\")[:10]:\n",
    "    title = balise.text #récupérer le texte\n",
    "    title = title.replace(\"\\xa0\",\"\") #enlever les symboles\n",
    "    top_titles.append(title) #ajouter le titre à la liste créée vide plus haut\n",
    "    \n",
    "#méthode 2: liste compréhension\n",
    "#top_titles = [ balise.text.replace(\"\\xa0\",\"\") for balise in soup.find_all(\"h3\")[:10] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#récupérer tous les chapeaux (descriptions) contenus dans la classe teaser__desc\n",
    "descriptions = soup.find_all(\"p\", \"teaser__desc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#récupérer les 10 premiers chapeaux (descriptions) contenus dans la classe teaser__desc\n",
    "#méthode 1: avec une boucle for\n",
    "\n",
    "top_descriptions = []\n",
    "\n",
    "for balise in soup.find_all(\"p\", \"teaser__desc\")[:10]:\n",
    "    description = balise.text #récupérer le texte\n",
    "    description = description.replace(\"\\xa0\",\"\") #enlever les symboles\n",
    "    top_descriptions.append(description) #ajouter le titre à la liste créée vide plus haut\n",
    "    \n",
    "#méthode 2: liste compréhension\n",
    "#top_titles = [ balise.text.replace(\"\\xa0\",\"\") for balise in soup.find_all(\"h3\")[:10] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 2: récupérer les liens des pages\n",
    "#soup.find_all(\"a\", \"teaser__link\")[0][\"href\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: récupérer la liste des 10 premiers articles du blog de Databird\n",
    "\n",
    "URL: https://www.data-bird.co/blog\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Le beau temps fait-il le bonheur ? (**)\n",
    "\n",
    "Les gens sont-ils plus heureux dans les pays ensoleillés ? Y a-t-il un lien entre le taux de dépression et le manque de soleil ? Intéressons-nous aux données pour répondre à cette question.\n",
    "\n",
    "<br>\n",
    "\n",
    "**_Hypothèse :_**\n",
    "\n",
    "Moins un pays compte d'heures d'ensoleillement, plus le taux d'individus déprimés est élevé.\n",
    "\n",
    "**_Data à collecter :_**\n",
    "* Noms de pays\n",
    "* Taux de personnes déprimées\n",
    "* Moyenne des heures d'ensoleillement \n",
    "\n",
    "**_Ressources à utiliser :_**\n",
    "* https://en.wikipedia.org/wiki/Epidemiology_of_depression\n",
    "* https://en.wikipedia.org/wiki/List_of_cities_by_sunshine_duration\n",
    "\n",
    "**_Questions :_**\n",
    "\n",
    "1. Scraper les 2 tables (taux d'individus déprimés et heures d'ensoleillement). (**)\n",
    "2. Nettoyer les données et calculer les moyennes annuelles d'ensoleillement par pays. (**)\n",
    "3. Fusionner les tables. (*)\n",
    "4. Afficher la corrélation à l'aide d'un scatter plot (`sns.scatterplot`). (*)\n",
    "\n",
    "<br>\n",
    "\n",
    "_**INDICES :**_\n",
    "\n",
    "Dans cet exercice, nous scrapons des tables sur un site web. Pour cette tâche, on peut utiliser BeautifulSoup comme d'habitude, ou utiliser la fonction pandas **`pd.read_html()`** qui est plus rapide (meilleure solution ici). Après avoir scrapé les données des articles, il reste simplement à les fusionner et à étudier la corrélation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zLlwVL0c3iN3"
   },
   "source": [
    "## Encyclopedia (***)\n",
    "\n",
    "On souhaite constituer une encyclopédie au cas où Wikipedia disparaîtrait. \n",
    "\n",
    "Pour cela, nous avons besoin d'une importante base d'articles pour que notre encyclopédie soit viable. Nous allons donc nous référer à l'article wikipédia qui liste les articles que toute bonne encyclopédie devrait posséder : https://fr.wikipedia.org/wiki/Wikip%C3%A9dia:Liste_d%27articles_que_toutes_les_encyclop%C3%A9dies_devraient_avoir.\n",
    "\n",
    "L'objectif de l'exercice est de sauvegarder cette liste d'articles au bon format, c'est-à-dire en préservant la hiérarchie des articles en sections et sous-sections. \n",
    "\n",
    "_Indice : Le dictionnaire pourrait être un bon format._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cr_UZPxw3iN7"
   },
   "source": [
    "## Choisir son futur ordinateur (***)\n",
    "\n",
    "**_[MAJ] Boulanger vient de modifier le code du site et de mettre en place des mesures anti-scraping. Cet exercice ne peut donc plus être effectué en l'état. MAJ à venir avec un autre site de vente d'informatique équivalent._**\n",
    "\n",
    "Supposons que vous souhaitiez changer d'ordinateur. Vous utilisez vos nouvelles compétences en scraping pour réaliser une étude des références disponibles sur le site **Boulanger**.\n",
    "\n",
    "* Scraper la première page de la catégorie  **[ultrabooks](https://www.boulanger.com/c/ultrabook-ultra-portable)**. Pour chaque produit de la page, récupérer son nom, son prix, la réduction s'il y en a une, le nombre d'étoiles et les caractéristiques principales. Sauvegarder ces informations dans un fichier csv.\n",
    "* Faire la même chose avec toutes les pages de la catégorie ultrabooks.\n",
    "\n",
    "_Indice : Partir de l'URL : https://www.boulanger.com/c/ultrabook-ultra-portable. La première question est réalisable sans ouvrir chaque page de produit._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BONUS : Le chasseur d'opportunités (*****, hacker level)\n",
    "\n",
    "Enregistrer le contenu de toutes les offres de poste contenant \"Data Analyst\" sur Welcome to the jungle.\n",
    "\n",
    "Cette tâche est rendue très compliquée par les méthodes anti-scraping mises en place par Welcome to the jungle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__Marche à suivre__*\n",
    "\n",
    "L'URL à utiliser pour cet exercice est : \n",
    "https://www.welcometothejungle.com/fr/jobs?page=1&refinementList%5Bprofession_name.fr.Tech%5D%5B%5D=Data%20Analysis&refinementList%5Bcontract_type_names.fr%5D%5B%5D=CDI\n",
    "\n",
    "On peut remarquer que : \n",
    "\n",
    "1. Il y a plusieurs pages de résultats, que l'on peut parcourir simplement **en changeant `page=k` dans l'URL**. Il y a **30 postes proposés par page** de résultats.\n",
    "<br><br>\n",
    "2. Welcome to the jungle a implémenté des mesures anti-scraping. En particulier, une partie du HTML est cachée lorsque l'on requête la page avec  `requests`. Il est indispensable de **commencer à scroller** la page pour lancer le code JavaScript qui révèle le contenu caché.\n",
    "<br><br>\n",
    "$\\rightarrow$ Pour résoudre ce problème, on ne peut se contenter de BeautifulSoup. Il faut **Simuler le comportement d'une vraie personne** qui parcourt la page avec sa souris, c'est donc **Selenium** qu'il nous faut\n",
    "<br><br>\n",
    "$\\rightarrow$ Voilà une fonction qui permet de simuler un scroll de page jusqu'à la ième offre d'emploi :\n",
    "<br><br>\n",
    "```\n",
    "def scroll(driver, i):\n",
    "        scroll_delta = int(250)\n",
    "        scroll_delta += 140*i\n",
    "        driver.execute_script(\"window.scrollBy(0, \"+ str(scroll_delta) + \")\")\n",
    "```\n",
    "<br>\n",
    "\n",
    "3. Une autre mesure anti-scraping concerne les noms de classes, les ids et même les liens vers des images dans le code HTML. Tous ces noms sont aléatoires (ex:`class=\"sc-1flb27e-5 cdtiMs\"`) et changent à chaque chargement de la page.<br><br>\n",
    "$\\rightarrow$ Une bonne nouvelle quand même : toutes les classes ne sont pas aléatoires, certaines restent fixes. Pour les noms aléatoires, certaines lettres du nom sont fixes également. On peut donc toujours utiliser des similarités pour désigner certains tags spécifiques (ex : le tag header, contenant le nombre total de résultats, commence toujours par \"hd\").<br><br>\n",
    "$\\rightarrow$ Pour exploiter cette faille, il est conseillé d'utiliser la méthode Selenium **`find_elements_by_css_selector()`** pour désigner des tags précis, car cette méthode permet de d'identifier un tag par un texte partiel (ex: `driver.find_elements_by_css_selector(\"header[class^='hd']\")` pour toutes les classes de headers qui commencent par \"hd\").\n",
    "<br><br>\n",
    "4. Au bout du compte, on souhaite sauvegarder le contenu de chaque offre d'emploi dans un fichier .txt.<br><br>\n",
    "$\\rightarrow$ Il va donc falloir cliquer sur chaque offre d'emploi avec la méthode **`.click()`** de Selenium. Pour chaque offre d'emploi, le contenu de l'offre est stocké dans un dictionnaire à l'intérieur d'un tag `<script>`. On peut utiliser la méthode  **`json.loads()`** pour manipuler ce dictionnaire. On peut finalement l'enregistrer en .txt avec les fonctions **`open()`** et **`.write`**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T11:08:46.802268Z",
     "start_time": "2021-03-31T11:08:46.505083Z"
    }
   },
   "outputs": [],
   "source": [
    "# importations nécessaires\n",
    "\n",
    "import time\n",
    "import json\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les cellules ci-dessous donnent des indications pour se lancer dans la marche à suivre présentée plus haut. Ce n'est qu'une proposition, et sans doute pas la seule méthode..!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fonction pour ouvrir une page\n",
    "####\n",
    "\n",
    "# fonction pour scroller jusqu'à une offre d'emploi \n",
    "####\n",
    "\n",
    "# fonction pour obtenir le nombre d'offres sur la page\n",
    "####\n",
    "\n",
    "# fonction pour cliquer sur une offre d'emploi \n",
    "####\n",
    "\n",
    "# function pour parser une page, extraire son contenu et l'enregistrer en .txt\n",
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# création du driver Selenium\n",
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ouverture de la page récupération du nombre d'offres\n",
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boucle de scraping \n",
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Jour 16 – Exercices.ipynb",
   "provenance": []
  },
  "gist": {
   "data": {
    "description": "x",
    "public": true
   },
   "id": ""
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
